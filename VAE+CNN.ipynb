{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE+CNN Model Over Generated Lattices (9x9x9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need json and pandas to open up our data that was generated by spinglass_metropolis.c++ and format for our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('data.json')\n",
    "\n",
    "# data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare an empty list to store the pairs\n",
    "# pairs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The for loop below is meant to label each lattice that is held witihn our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over the configurations\n",
    "# for config in data['Configuration'].values():\n",
    "#     for s in config.values():\n",
    "#         for temp, lattice_list in s['Temp'].items():\n",
    "#             # Convert the temperature to a float and determine the label\n",
    "#             temp = float(temp)\n",
    "#             label = 1 if temp < 1.1 else 0\n",
    "\n",
    "#             # Iterate over each 9x9x9 array in the lattice list\n",
    "#             for lattice in lattice_list:\n",
    "                \n",
    "#                 # Add the pair to the list\n",
    "#                 pairs.append((temp, lattice, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the reformatted data into a pandas frame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the list of pairs to a DataFrame\n",
    "# df = pd.DataFrame(pairs, columns=['Temperature', 'Lattice', 'Label'])\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data frame.\n",
    "df = pd.read_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert the 'Lattice' column from string representation of list to numpy array, ensure values are float32 for tensorflow...\n",
    "df['Lattice'] = df['Lattice'].apply(lambda x: np.array(ast.literal_eval(x)).astype('float32'))\n",
    "\n",
    "# Get unique temperatures\n",
    "temperatures = df['Temperature'].unique()\n",
    "\n",
    "# Create a dictionary of DataFrames for each temperature\n",
    "dfs = {temp: df[df['Temperature'] == temp] for temp in temperatures}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN_TEST_SPLIT DATA\n",
    "\n",
    "NOTE: Tensorflow wont work normally with our X_train, X_test, if we were to split our data without a bit of further processing because its an array of arrays. We need to make the training be in a stack and add another column so that the library can properly make tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to hold the train and test splits for each temperature\n",
    "splits = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "for temp in temperatures:\n",
    "    # Get the lattices for this temperature\n",
    "    X = dfs[temp]['Lattice'].values\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Store the splits in the dictionary\n",
    "    splits[temp] = (X_train, X_test)\n",
    "    \n",
    "    # Convert lists to numpy arrays and add an extra dimension for the 'channels' in our VAE+CNN\n",
    "    X_train = np.stack(X_train)[..., np.newaxis]\n",
    "    X_test = np.stack(X_test)[..., np.newaxis]\n",
    "\n",
    "    # Store the splits in the dictionary\n",
    "    splits[temp] = (X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in temperatures:\n",
    "    X_train, X_test = splits[temp]\n",
    "    print(\"Here is the sizes of train and test for this temperature: \", temp)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRUCTURE OF VAE+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "layers = keras.layers\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = self.create_encoder()\n",
    "        self.decoder = self.create_decoder()\n",
    "\n",
    "    def create_encoder(self):\n",
    "        inputs = layers.Input(shape=(9, 9, 9, 1))\n",
    "        x = layers.Conv3D(32, 3, activation=\"selu\", strides=2, padding=\"same\")(inputs)\n",
    "        x = layers.Conv3D(64, 3, activation=\"selu\", strides=2, padding=\"same\")(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(16, activation=\"selu\")(x)\n",
    "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
    "        return tf.keras.Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "\n",
    "    def create_decoder(self):\n",
    "        latent_inputs = layers.Input(shape=(self.latent_dim,))\n",
    "        x = layers.Dense(128 * 3 * 3 * 3, activation=\"selu\")(latent_inputs)  # Increase the size here\n",
    "        x = layers.Reshape((3, 3, 3, 128))(x)  # Now the total size matches\n",
    "        x = layers.Conv3DTranspose(64, 3, activation=\"selu\", strides=2, padding=\"valid\")(x)\n",
    "        x = layers.Conv3DTranspose(32, 3, activation=\"selu\", strides=1, padding=\"valid\")(x)\n",
    "        decoder_outputs = layers.Conv3DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "        return tf.keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    \n",
    "    def reparameterize(self, mean, log_var):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(log_var * .5) + mean\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstructed = self.decoder(z)\n",
    "        reconstructed = reconstructed * 2 - 1 #scale the output to be in range in the range [-1,1]\n",
    "        # Add KL divergence regularization loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the trained VAEs\n",
    "vaes = {}\n",
    "\n",
    "for temp, (X_train, X_test) in splits.items():\n",
    "    # Initialize a VAE\n",
    "    vae = VAE(latent_dim=3)\n",
    "\n",
    "    # Compile the VAE\n",
    "    vae.compile(optimizer='nadam', loss=tf.keras.losses.BinaryCrossentropy())\n",
    "\n",
    "    # Train the VAE\n",
    "    vae.fit(X_train, X_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Store the trained VAE\n",
    "    vaes[temp] = vae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to hold the means and variances\n",
    "means = {}\n",
    "variances = {}\n",
    "\n",
    "for temp, vae in vaes.items():\n",
    "    # Use the encoder to transform the lattices into the latent space\n",
    "    z_mean, z_log_var = vae.encoder.predict(splits[temp][0])  # Use the training data\n",
    "\n",
    "    # Compute the mean and variance of the Gaussian distribution\n",
    "    means[temp] = np.mean(z_mean, axis=0)\n",
    "    variances[temp] = np.mean(np.exp(z_log_var), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the means\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(len(list(means.values())[0])):  # Loop over each dimension of the latent space\n",
    "    plt.plot(list(means.keys()), [m[i] for m in means.values()], marker='o', label=f'Mean Dimension {i+1}')\n",
    "plt.title('Mean of the Gaussian Distribution as a Function of Temperature')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Mean')\n",
    "plt.legend()  # Add a legend\n",
    "plt.show()\n",
    "\n",
    "# Plot the variances\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(len(list(variances.values())[0])):  # Loop over each dimension of the latent space\n",
    "    plt.plot(list(variances.keys()), [v[i] for v in variances.values()], marker='o', label=f'Variance Dimension {i+1}')\n",
    "plt.title('Variance of the Gaussian Distribution as a Function of Temperature')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()  # Add a legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Get the temperatures\n",
    "temps = list(means.keys())\n",
    "\n",
    "# Get the means for each dimension\n",
    "variance_dim1 = [m[0] for m in variances.values()]\n",
    "varience_dim2 = [m[1] for m in variances.values()]\n",
    "variance_dim3 = [m[2] for m in variances.values()]\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "ax.scatter(temps, means_dim1, means_dim2, c=means_dim3)\n",
    "\n",
    "ax.set_xlabel('Temperature')\n",
    "ax.set_ylabel('Mean Dimension 1')\n",
    "ax.set_zlabel('Mean Dimension 2')\n",
    "plt.title('Varience of the Gaussian Distribution as a Function of Temperature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation?\n",
    "above we saw that there was a behavior that was picked up within the lattices but how do we know if this is not just coincidence? Lets try to redo it but this time shuffling our data and seeing if we still see something similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Number of different shuffles you want to try\n",
    "N = 5\n",
    "\n",
    "# Initialize a list to hold all the VAEs\n",
    "all_vaes = []\n",
    "\n",
    "# Initialize a list to hold the train and test splits made to later extract the latent variables...\n",
    "all_splits = []\n",
    "\n",
    "for i in range(N):\n",
    "    # Initialize a dictionary to hold the train and test splits for each temperature\n",
    "    splits = {}\n",
    "    # Initialize a dictionary to hold the trained VAEs\n",
    "    vaes = {}\n",
    "\n",
    "    for temp in temperatures:\n",
    "        # Get the lattices for this temperature\n",
    "        X = dfs[temp]['Lattice'].values\n",
    "\n",
    "        # Shuffle the data\n",
    "        X = shuffle(X, random_state=i)\n",
    "\n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Convert lists to numpy arrays and add an extra dimension for the 'channels' in our VAE+CNN\n",
    "        X_train = np.stack(X_train)[..., np.newaxis]\n",
    "        X_test = np.stack(X_test)[..., np.newaxis]\n",
    "\n",
    "        # Store the splits in the dictionary\n",
    "        splits[temp] = (X_train, X_test)\n",
    "\n",
    "        # Initialize a VAE\n",
    "        vae = VAE(latent_dim=3)\n",
    "\n",
    "        # Compile the VAE\n",
    "        vae.compile(optimizer='nadam', loss=tf.keras.losses.BinaryCrossentropy())\n",
    "\n",
    "        # Train the VAE\n",
    "        vae.fit(X_train, X_train, epochs=10, batch_size=32)\n",
    "\n",
    "        # Store the trained VAE\n",
    "        vaes[temp] = vae\n",
    "\n",
    "    # Add the trained VAEs to the list\n",
    "    all_vaes.append(vaes)\n",
    "    all_splits.append(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold the means and variances for all VAEs\n",
    "all_means = []\n",
    "all_variances = []\n",
    "\n",
    "for i in range(N):\n",
    "    # Get the VAEs and splits for this shuffle\n",
    "    vaes = all_vaes[i]\n",
    "    splits = splits_list[i]\n",
    "\n",
    "    # Initialize dictionaries to hold the means and variances\n",
    "    means = {}\n",
    "    variances = {}\n",
    "\n",
    "    for temp, vae in vaes.items():\n",
    "        # Use the encoder to transform the lattices into the latent space\n",
    "        z_mean, z_log_var = vae.encoder.predict(splits[temp][0])  # Use the training data\n",
    "\n",
    "        # Compute the mean and variance of the Gaussian distribution\n",
    "        means[temp] = np.mean(z_mean, axis=0)\n",
    "        variances[temp] = np.mean(np.exp(z_log_var), axis=0)\n",
    "\n",
    "    # Add the means and variances to the lists\n",
    "    all_means.append(means)\n",
    "    all_variances.append(variances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create a directory to save the plots\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Loop over each shuffle\n",
    "for i in range(N):\n",
    "    # Get the means and variances for this shuffle\n",
    "    means = all_means[i]\n",
    "    variances = all_variances[i]\n",
    "\n",
    "    # Plot the means\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for j in range(len(list(means.values())[0])):  # Loop over each dimension of the latent space\n",
    "        plt.plot(list(means.keys()), [m[j] for m in means.values()], marker='o', label=f'Mean Dimension {j+1}')\n",
    "    plt.title(f'Mean of the Gaussian Distribution as a Function of Temperature (Shuffle {i+1})')\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Mean')\n",
    "    plt.legend()  # Add a legend\n",
    "    plt.savefig(f\"plots/means_shuffle_{i+1}.png\")  # Save the plot\n",
    "    plt.close()\n",
    "\n",
    "    # Plot the variances\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for j in range(len(list(variances.values())[0])):  # Loop over each dimension of the latent space\n",
    "        plt.plot(list(variances.keys()), [v[j] for v in variances.values()], marker='o', label=f'Variance Dimension {j+1}')\n",
    "    plt.title(f'Variance of the Gaussian Distribution as a Function of Temperature (Shuffle {i+1})')\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()  # Add a legend\n",
    "    plt.savefig(f\"plots/variances_shuffle_{i+1}.png\")  # Save the plot\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
